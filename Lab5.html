<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Laboratorio 5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="Lab5_files/libs/clipboard/clipboard.min.js"></script>
<script src="Lab5_files/libs/quarto-html/quarto.js"></script>
<script src="Lab5_files/libs/quarto-html/popper.min.js"></script>
<script src="Lab5_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Lab5_files/libs/quarto-html/anchor.min.js"></script>
<link href="Lab5_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Lab5_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Lab5_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Lab5_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Lab5_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Laboratorio 5</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>En este laboratorio, estaremos repasando los conceptos de Atención y Transformadores. Buscaremos acercanos a la implementación del paper <a href="https://arxiv.org/abs/1706.03762">“Attention is All you Need”</a>. Por ello, todas las imagenes que veremos aca son del paper, a menos que se indique lo contrario.</p>
<p>Al igual que en laboratorios anteriores, para este laboratorio estaremos usando una herramienta para Jupyter Notebooks que facilitará la calificación, no solo asegurándo que ustedes tengan una nota pronto sino también mostrandoles su nota final al terminar el laboratorio.</p>
<p>De nuevo me discupo si algo no sale bien, seguiremos mejorando conforme vayamos iterando. Siempre pido su comprensión y colaboración si algo no funciona como debería.</p>
<p>Al igual que en el laboratorio pasado, estaremos usando la librería de Dr John Williamson et al de la University of Glasgow, además de ciertas piezas de código de Dr Bjorn Jensen de su curso de Introduction to Data Science and System de la University of Glasgow para la visualización de sus calificaciones.</p>
<p><strong>NOTA:</strong> Ahora tambien hay una tercera dependecia que se necesita instalar. Ver la celda de abajo por favor</p>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:35.054364Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:35.041327Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;acb39d7a22bd30fe2f43269db8fc178c&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-d337e90c7330c914&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Una vez instalada la librería por favor, recuerden volverla a comentar.</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#!pip install -U --force-reinstall --no-cache https://github.com/johnhw/jhwutils/zipball/master</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install scikit-image</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install -U --force-reinstall --no-cache https://github.com/AlbertS789/lautils/zipball/master</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Collecting scikit-image
  Downloading scikit_image-0.21.0-cp311-cp311-win_amd64.whl (22.8 MB)
                                              0.0/22.8 MB ? eta -:--:--
                                              0.0/22.8 MB 1.4 MB/s eta 0:00:17
                                              0.0/22.8 MB 1.4 MB/s eta 0:00:17
                                             0.1/22.8 MB 655.4 kB/s eta 0:00:35
                                             0.1/22.8 MB 819.2 kB/s eta 0:00:28
                                             0.2/22.8 MB 981.9 kB/s eta 0:00:23
                                             0.2/22.8 MB 962.7 kB/s eta 0:00:24
                                              0.4/22.8 MB 1.3 MB/s eta 0:00:18
                                              0.5/22.8 MB 1.5 MB/s eta 0:00:15
     -                                        0.6/22.8 MB 1.5 MB/s eta 0:00:15
     -                                        0.9/22.8 MB 2.0 MB/s eta 0:00:11
     -                                        1.0/22.8 MB 2.2 MB/s eta 0:00:10
     -                                        1.1/22.8 MB 2.1 MB/s eta 0:00:11
     --                                       1.3/22.8 MB 2.2 MB/s eta 0:00:10
     --                                       1.6/22.8 MB 2.6 MB/s eta 0:00:09
     ---                                      2.1/22.8 MB 3.0 MB/s eta 0:00:07
     ----                                     2.6/22.8 MB 3.5 MB/s eta 0:00:06
     -----                                    2.9/22.8 MB 3.7 MB/s eta 0:00:06
     -----                                    3.1/22.8 MB 3.8 MB/s eta 0:00:06
     -----                                    3.3/22.8 MB 3.7 MB/s eta 0:00:06
     -----                                    3.4/22.8 MB 3.7 MB/s eta 0:00:06
     ------                                   3.6/22.8 MB 3.7 MB/s eta 0:00:06
     ------                                   3.8/22.8 MB 3.7 MB/s eta 0:00:06
     -------                                  4.2/22.8 MB 3.9 MB/s eta 0:00:05
     --------                                 4.6/22.8 MB 4.2 MB/s eta 0:00:05
     --------                                 4.9/22.8 MB 4.3 MB/s eta 0:00:05
     ---------                                5.3/22.8 MB 4.4 MB/s eta 0:00:04
     ----------                               5.8/22.8 MB 4.7 MB/s eta 0:00:04
     ----------                               6.3/22.8 MB 4.8 MB/s eta 0:00:04
     -----------                              6.8/22.8 MB 5.1 MB/s eta 0:00:04
     ------------                             7.1/22.8 MB 5.1 MB/s eta 0:00:04
     -------------                            7.5/22.8 MB 5.2 MB/s eta 0:00:03
     -------------                            7.9/22.8 MB 5.3 MB/s eta 0:00:03
     --------------                           8.5/22.8 MB 5.6 MB/s eta 0:00:03
     ---------------                          9.0/22.8 MB 5.7 MB/s eta 0:00:03
     ----------------                         9.3/22.8 MB 5.7 MB/s eta 0:00:03
     ----------------                         9.6/22.8 MB 5.8 MB/s eta 0:00:03
     -----------------                        10.2/22.8 MB 5.9 MB/s eta 0:00:03
     ------------------                       10.6/22.8 MB 7.1 MB/s eta 0:00:02
     -------------------                      10.9/22.8 MB 7.4 MB/s eta 0:00:02
     -------------------                      11.2/22.8 MB 7.4 MB/s eta 0:00:02
     --------------------                     11.6/22.8 MB 7.8 MB/s eta 0:00:02
     --------------------                     11.8/22.8 MB 7.8 MB/s eta 0:00:02
     ---------------------                    12.1/22.8 MB 7.6 MB/s eta 0:00:02
     ---------------------                    12.4/22.8 MB 7.6 MB/s eta 0:00:02
     ----------------------                   12.5/22.8 MB 7.4 MB/s eta 0:00:02
     ----------------------                   13.0/22.8 MB 7.4 MB/s eta 0:00:02
     -----------------------                  13.3/22.8 MB 7.4 MB/s eta 0:00:02
     -----------------------                  13.6/22.8 MB 7.8 MB/s eta 0:00:02
     ------------------------                 13.9/22.8 MB 7.9 MB/s eta 0:00:02
     ------------------------                 14.2/22.8 MB 7.9 MB/s eta 0:00:02
     -------------------------                14.5/22.8 MB 7.8 MB/s eta 0:00:02
     -------------------------                14.8/22.8 MB 7.7 MB/s eta 0:00:02
     --------------------------               15.0/22.8 MB 7.7 MB/s eta 0:00:02
     ---------------------------              15.4/22.8 MB 7.6 MB/s eta 0:00:01
     ---------------------------              15.7/22.8 MB 7.6 MB/s eta 0:00:01
     ----------------------------             16.0/22.8 MB 7.4 MB/s eta 0:00:01
     ----------------------------             16.3/22.8 MB 7.4 MB/s eta 0:00:01
     -----------------------------            16.6/22.8 MB 7.3 MB/s eta 0:00:01
     -----------------------------            16.9/22.8 MB 7.2 MB/s eta 0:00:01
     ------------------------------           17.1/22.8 MB 7.1 MB/s eta 0:00:01
     ------------------------------           17.4/22.8 MB 7.1 MB/s eta 0:00:01
     -------------------------------          17.7/22.8 MB 7.0 MB/s eta 0:00:01
     -------------------------------          18.0/22.8 MB 7.0 MB/s eta 0:00:01
     --------------------------------         18.3/22.8 MB 7.0 MB/s eta 0:00:01
     --------------------------------         18.6/22.8 MB 6.7 MB/s eta 0:00:01
     ---------------------------------        18.9/22.8 MB 6.6 MB/s eta 0:00:01
     ---------------------------------        19.1/22.8 MB 6.6 MB/s eta 0:00:01
     ----------------------------------       19.5/22.8 MB 6.6 MB/s eta 0:00:01
     ----------------------------------       19.8/22.8 MB 6.5 MB/s eta 0:00:01
     -----------------------------------      20.0/22.8 MB 6.5 MB/s eta 0:00:01
     -----------------------------------      20.3/22.8 MB 6.4 MB/s eta 0:00:01
     ------------------------------------     20.6/22.8 MB 6.4 MB/s eta 0:00:01
     ------------------------------------     20.9/22.8 MB 6.4 MB/s eta 0:00:01
     -------------------------------------    21.2/22.8 MB 6.4 MB/s eta 0:00:01
     -------------------------------------    21.5/22.8 MB 6.3 MB/s eta 0:00:01
     --------------------------------------   21.8/22.8 MB 6.3 MB/s eta 0:00:01
     --------------------------------------   22.1/22.8 MB 6.3 MB/s eta 0:00:01
     ---------------------------------------  22.4/22.8 MB 6.3 MB/s eta 0:00:01
     ---------------------------------------  22.7/22.8 MB 6.3 MB/s eta 0:00:01
     ---------------------------------------  22.8/22.8 MB 6.4 MB/s eta 0:00:01
     ---------------------------------------  22.8/22.8 MB 6.4 MB/s eta 0:00:01
     ---------------------------------------- 22.8/22.8 MB 6.0 MB/s eta 0:00:00
Requirement already satisfied: numpy&gt;=1.21.1 in c:\users\aleja\documents\github\lab5_dl\lab5\lib\site-packages (from scikit-image) (1.25.2)
Collecting scipy&gt;=1.8 (from scikit-image)
  Using cached scipy-1.11.2-cp311-cp311-win_amd64.whl (44.0 MB)
Collecting networkx&gt;=2.8 (from scikit-image)
  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)
                                              0.0/2.1 MB ? eta -:--:--
     ---                                      0.2/2.1 MB 6.3 MB/s eta 0:00:01
     -----------                              0.6/2.1 MB 7.7 MB/s eta 0:00:01
     --------------------                     1.0/2.1 MB 8.3 MB/s eta 0:00:01
     ---------------------------              1.4/2.1 MB 8.3 MB/s eta 0:00:01
     ------------------------------------     1.9/2.1 MB 8.7 MB/s eta 0:00:01
     ---------------------------------------- 2.1/2.1 MB 7.8 MB/s eta 0:00:00
Collecting pillow&gt;=9.0.1 (from scikit-image)
  Using cached Pillow-10.0.0-cp311-cp311-win_amd64.whl (2.5 MB)
Collecting imageio&gt;=2.27 (from scikit-image)
  Downloading imageio-2.31.1-py3-none-any.whl (313 kB)
                                              0.0/313.2 kB ? eta -:--:--
     ----------------------------           235.5/313.2 kB 4.8 MB/s eta 0:00:01
     -------------------------------------- 313.2/313.2 kB 3.9 MB/s eta 0:00:00
Collecting tifffile&gt;=2022.8.12 (from scikit-image)
  Downloading tifffile-2023.8.12-py3-none-any.whl (220 kB)
                                              0.0/221.0 kB ? eta -:--:--
     ------------------------------------   215.0/221.0 kB 6.6 MB/s eta 0:00:01
     -------------------------------------- 221.0/221.0 kB 4.5 MB/s eta 0:00:00
Collecting PyWavelets&gt;=1.1.1 (from scikit-image)
  Downloading PyWavelets-1.4.1-cp311-cp311-win_amd64.whl (4.2 MB)
                                              0.0/4.2 MB ? eta -:--:--
     ----                                     0.5/4.2 MB 10.5 MB/s eta 0:00:01
     -----------                              1.2/4.2 MB 12.3 MB/s eta 0:00:01
     -----------------                        1.8/4.2 MB 12.7 MB/s eta 0:00:01
     ----------------------                   2.4/4.2 MB 12.6 MB/s eta 0:00:01
     -----------------------------            3.1/4.2 MB 13.2 MB/s eta 0:00:01
     -----------------------------------      3.7/4.2 MB 13.2 MB/s eta 0:00:01
     ---------------------------------------  4.2/4.2 MB 13.3 MB/s eta 0:00:01
     ---------------------------------------- 4.2/4.2 MB 12.1 MB/s eta 0:00:00
Requirement already satisfied: packaging&gt;=21 in c:\users\aleja\documents\github\lab5_dl\lab5\lib\site-packages (from scikit-image) (23.1)
Collecting lazy_loader&gt;=0.2 (from scikit-image)
  Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)
Installing collected packages: tifffile, scipy, PyWavelets, pillow, networkx, lazy_loader, imageio, scikit-image
Successfully installed PyWavelets-1.4.1 imageio-2.31.1 lazy_loader-0.3 networkx-3.1 pillow-10.0.0 scikit-image-0.21.0 scipy-1.11.2 tifffile-2023.8.12
Collecting https://github.com/AlbertS789/lautils/zipball/master
  Downloading https://github.com/AlbertS789/lautils/zipball/master
     - 0 bytes ? 0:00:00
     - 4.2 kB ? 0:00:00
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Building wheels for collected packages: lautils
  Building wheel for lautils (pyproject.toml): started
  Building wheel for lautils (pyproject.toml): finished with status 'done'
  Created wheel for lautils: filename=lautils-1.0-py3-none-any.whl size=2833 sha256=97b134b8ab476460d9c115c272e3198ecc9e92c8fcb50aa597adc5a632f0e303
  Stored in directory: C:\Users\aleja\AppData\Local\Temp\pip-ephem-wheel-cache-82wtg5i3\wheels\1a\50\ba\b3ceb937949f5894a896b68af5b5fdb598e50244141063e4db
Successfully built lautils
Installing collected packages: lautils
Successfully installed lautils-1.0</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
[notice] A new release of pip is available: 23.1.2 -&gt; 23.2.1
[notice] To update, run: python.exe -m pip install --upgrade pip

[notice] A new release of pip is available: 23.1.2 -&gt; 23.2.1
[notice] To update, run: python.exe -m pip install --upgrade pip</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-20T02:07:40.915826Z&quot;,&quot;start_time&quot;:&quot;2023-08-20T02:07:28.437774Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;d930305954bab29143585b31039fbdc8&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-5829eb3649aa440c&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">#from IPython import display</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">#from base64 import b64decode</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Other imports</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> unittest.mock <span class="im">import</span> patch</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> uuid <span class="im">import</span> getnode <span class="im">as</span> get_mac</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jhwutils.checkarr <span class="im">import</span> array_hash, check_hash, check_scalar, check_string, array_hash, _check_scalar</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jhwutils.image_audio <span class="im">as</span> ia</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jhwutils.tick <span class="im">as</span> tick</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lautils.gradeutils <span class="im">import</span> new_representation, hex_to_float, compare_numbers, compare_lists_by_percentage, calculate_coincidences_percentage</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co">###</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>tick.reset_marks()</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:36.250534Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:36.236019Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;0f658c10aeff9ccd90992ac2552037da&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-559b86f87afcf85c&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Seeds</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>seed_ <span class="op">=</span> <span class="dv">2023</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:36.266407Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:36.252492Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;476b5f908e1f909fe0f9c55eb1a3a0bb&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-3985ef836649b844&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Celda escondida para utlidades necesarias, por favor NO edite esta celda</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="información-del-estudiante-en-dos-variables" class="level6">
<h6 class="anchored" data-anchor-id="información-del-estudiante-en-dos-variables">Información del estudiante en dos variables</h6>
<ul>
<li>carne_1 : un string con su carne (e.g.&nbsp;“12281”), debe ser de al menos 5 caracteres.</li>
<li>firma_mecanografiada_1: un string con su nombre (e.g.&nbsp;“Albero Suriano”) que se usará para la declaracion que este trabajo es propio (es decir, no hay plagio)</li>
<li>carne_2 : un string con su carne (e.g.&nbsp;“12281”), debe ser de al menos 5 caracteres.</li>
<li>firma_mecanografiada_2: un string con su nombre (e.g.&nbsp;“Albero Suriano”) que se usará para la declaracion que este trabajo es propio (es decir, no hay plagio)</li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:36.282242Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:36.269397Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;449f77f65624316127652ef18b89810f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-f915e9ab272f87d9&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># carne_1 = </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># firma_mecanografiada_1 = </span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># carne_2 = </span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># firma_mecanografiada_2 = </span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>carne_1 <span class="op">=</span> <span class="st">"161250"</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>firma_mecanografiada_1 <span class="op">=</span> <span class="st">"Manuel Archila"</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>carne_2 <span class="op">=</span> <span class="st">"161250"</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>firma_mecanografiada_2 <span class="op">=</span> <span class="st">"Manuel Archila"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:55:37.986608Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:55:37.976631Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;58edc66cd77ae9ecbc7886dd5e9ec45a&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-51da86f6ac657e33&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Deberia poder ver dos checkmarks verdes [0 marks], que indican que su información básica está OK </span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">0</span>): </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">len</span>(carne_1)<span class="op">&gt;=</span><span class="dv">5</span> <span class="kw">and</span> <span class="bu">len</span>(carne_2)<span class="op">&gt;=</span><span class="dv">5</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">0</span>):  </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">len</span>(firma_mecanografiada_1)<span class="op">&gt;</span><span class="dv">0</span> <span class="kw">and</span> <span class="bu">len</span>(firma_mecanografiada_2)<span class="op">&gt;</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"0"}--> 
         ✓ [0 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"0"}--> 
         ✓ [0 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="introducción" class="level3">
<h3 class="anchored" data-anchor-id="introducción">Introducción</h3>
<p>Similar al modelo Seq2Seq, el modelo de Transformer no usará recurrencias, ni tampoco capaz convolucionales. En su lugar, el modelo está hecho meramente con capaz lineales, mecanismos de atención y normalización.</p>
<p>Una de las variantes más populares de los Transformadores es BERT (Bidrectional Encoder Representations from Transformers) y versiones pre-entrenadas de BERT que son comunmente usadas para sistituir capaz de embedding (y otras cosas más) en modelos de NLP.</p>
<p>Cabe descatar algunas diferencias entre la implentación que haremos y la del paper:</p>
<ul>
<li>Usaremos un positional encoding aprendido y no uno estático</li>
<li>Usaremos un optimizador estándar Adam con un learning rate estático, en lugar de uno con warm-up y cool-down</li>
<li>No usaremos label smoothing</li>
</ul>
<p>Se consideran estas modificaciones a finalidad de hacer una implementación que se acerque a como BERT suele ser seteado.</p>
<p>Consideren que para esta parte estaremos usando el mismo dataset que usamos para la segunda parte del laboratorio pasado. Por ende, sugiero que usen el mismo venv que usaron para esa parte.</p>
<p><strong>Créditos:</strong> Esta parte de este laboratorio está tomado y basado en uno de los repositorios de Ben Trevett</p>
</section>
<section id="preparando-la-data" class="level3">
<h3 class="anchored" data-anchor-id="preparando-la-data">Preparando la Data</h3>
<p>Como la otra vez, volvemos a empezar importando las librerías necesarias. Así también seteamos la Seed para asegurar que las calificaciones sean consistentes.</p>
<p>Despues, al igual que en el lab anterior, haremos el tokenizador. Así mismo definimos mismo Field de la ultima vez con la diferencia menor que ahora estaremos pasando batches de datos, por ende usaremos el parámetr “batch_first=True”</p>
<p>Despues cargaremos el mismo dataset de la ultima vez “Multi30K” para construir nuestro vocabulario. Donde se cargan los sets de <code>train_data</code>, <code>valid_data</code> y <code>test_data</code>, hagan los cambios necesarios para cargar los datos como lo hicieron la última vez. <strong>Siéntase libre de hacer copy-paste de lo que hicieron en el lab4.</strong></p>
<p>Finalmente, definiremos el <code>device</code> con el que estaremos trabajando. <strong>Se recomienda usar CUDA</strong>. Por otro lado, recuerden que tienen <strong>disponible el laboratorio del CIT-411</strong> para que lo usen en el período de clase de los días lunes. En las máquinas de este laboratorio pueden usar CUDA y deberían ser más rápidas que los tiempos mostrados en este Notebook.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:37.372076Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:36.299380Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;abdca458f5a8958ecea94fa2cb1bdaa9&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-210657b016e6a12a&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchtext</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.datasets <span class="im">import</span> Multi30k</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.data <span class="im">import</span> Field, BucketIterator</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.ticker <span class="im">as</span> ticker</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:37.387682Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:37.373883Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;c9c9be35cbf891ae86c53e908431ab84&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-2f6d6f39aafb1a4c&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>random.seed(seed_)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed_)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed(seed_)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:38.599496Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:37.390483Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;f0f7eb82159e6405ce10ecefb952da50&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-d20eb45ab17eae05&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>spacy_de <span class="op">=</span> spacy.load(<span class="st">'de_core_news_sm'</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>spacy_en <span class="op">=</span> spacy.load(<span class="st">'en_core_web_sm'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:38.614917Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:38.601332Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;522db5a69f64e2c3833194bdf8a9a800&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-5b6d056a707bcf13&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_de(text):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [tok.text <span class="cf">for</span> tok <span class="kw">in</span> spacy_de.tokenizer(text)]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_en(text):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [tok.text <span class="cf">for</span> tok <span class="kw">in</span> spacy_en.tokenizer(text)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:38.631146Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:38.617662Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;1dcf493f9c14184289f72c56bd928300&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-785350782f36fd67&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Noten el uso de batch_first</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>SRC <span class="op">=</span> Field(tokenize <span class="op">=</span> tokenize_de, </span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>            init_token <span class="op">=</span> <span class="st">'&lt;sos&gt;'</span>, </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>            eos_token <span class="op">=</span> <span class="st">'&lt;eos&gt;'</span>, </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>            lower <span class="op">=</span> <span class="va">True</span>, </span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>            batch_first <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>TRG <span class="op">=</span> Field(tokenize <span class="op">=</span> tokenize_en, </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>            init_token <span class="op">=</span> <span class="st">'&lt;sos&gt;'</span>, </span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>            eos_token <span class="op">=</span> <span class="st">'&lt;eos&gt;'</span>, </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>            lower <span class="op">=</span> <span class="va">True</span>, </span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>            batch_first <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:41.871179Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:38.633309Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;1324dd6902ef82ce34f900b1203d8447&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-2a890b2bc340545f&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">#train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), </span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co">#                                                    fields = (SRC, TRG))</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># En esta sección hagan lo mismo que hicieron en el lab4 para cargar</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># los datos necesarios por favor</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>train_data, valid_data, test_data <span class="op">=</span> Multi30k.splits(exts <span class="op">=</span> (<span class="st">'.de'</span>, <span class="st">'.en'</span>), </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>                                                    fields <span class="op">=</span> (SRC, TRG),</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>                                                    root <span class="op">=</span> <span class="st">'./data'</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:42.026151Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:41.873212Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;df746de24ea6c3719ef4f75f3f68258f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-833fb478d4387974&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>SRC.build_vocab(train_data, min_freq <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>TRG.build_vocab(train_data, min_freq <span class="op">=</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:42.042110Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:42.031137Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;7e8efd8e6a14f83de738c082bd831c30&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-a92e23f0886d7aa9&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Se recomienda el uso de CUDA</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>cuda</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:42.058196Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:42.043107Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;7c9eba4a4c229e7c6402c775368b366b&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-a9814ed615455a51&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="16">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Definimos el tamaño del batch y creamos iteradores</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>train_iterator, valid_iterator, test_iterator <span class="op">=</span> BucketIterator.splits(</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    (train_data, valid_data, test_data), </span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>     batch_size <span class="op">=</span> BATCH_SIZE,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>     device <span class="op">=</span> device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="construyendo-el-modelo" class="level3">
<h3 class="anchored" data-anchor-id="construyendo-el-modelo">Construyendo el Modelo</h3>
<p>A continuación, construiremos el modelo. Al igual que los notebook anteriores, se compone de un <em>encoder</em> y un <em>decoder</em>, con el encoder <em>codificando</em> la oración de entrada/fuente (en alemán) en <em>vector de contexto</em> y el decpder luego <em>decodificando</em> este vector de contexto para generar nuestra oración de salida/objetivo (en inglés)</p>
<section id="encoder" class="level4">
<h4 class="anchored" data-anchor-id="encoder">Encoder</h4>
<p>El codificador de Transformer no intenta comprimir la oración fuente completa, <span class="math inline">\(X = (x_1, ... ,x_n)\)</span>, en un solo vector de contexto, <span class="math inline">\(z\)</span>. En su lugar, produce una secuencia de vectores de contexto, <span class="math inline">\(Z = (z_1, ... , z_n)\)</span>. Entonces, si nuestra secuencia de entrada fuera de 5 tokens, tendríamos <span class="math inline">\(Z = (z_1, z_2, z_3, z_4, z_5)\)</span>.</p>
<p>¿Por qué llamamos a esto una secuencia de vectores de contexto y no una secuencia de estados ocultos? Un estado oculto en el tiempo <span class="math inline">\(t\)</span> en un RNN solo ha visto tokens <span class="math inline">\(x_t\)</span> y todos los tokens anteriores. Sin embargo, cada vector de contexto aquí ha visto todos los tokens en todas las posiciones dentro de la secuencia de entrada.</p>
<p><img src="https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/49df8404d938a6edbf729876405558cc2c2b3013//assets/transformer-encoder.png" class="img-fluid"></p>
<p>Primero, los tokens se pasan a través de una capa de embedding estándar. Luego, como el modelo no tiene recurrencia, no tiene idea del orden de los tokens dentro de la secuencia. Resolvemos esto usando una segunda capa de embedding llamada <em>capa de positional embedding</em>. Esta es una capa de embedding estándar donde la entrada no es el token en sí, sino la posición del token dentro de la secuencia, comenzando con el primer token, el token <code>&lt;sos&gt;</code> (inicio de secuencia), en la posición 0. La posición embeddida tiene un tamaño de “vocabulario” de 100, lo que significa que nuestro modelo puede aceptar oraciones de hasta 100 tokens de largo. Esto se puede aumentar si queremos manejar oraciones más largas.</p>
<p>La implementación original de Transformer del documento Attention is All You Need no aprende embedding posicionales. En su lugar, utiliza una incrustación estática fija. Las arquitecturas modernas de Transformer, como BERT, usan embedding posicionales en su lugar, por lo que lo haremos asi en este laboratorio. Consulte <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding">esta</a> sección para obtener más información sobre las positional embedding utilizadas en el modelo Transformer original.</p>
<p>A continuación, los embedding de tokens y posicionales se suman por elementos para obtener un vector que contiene información sobre el token y también su posición en la secuencia. Sin embargo, antes de que se sumen, las incrustaciones de tokens se multiplican por un factor de escala que es <span class="math inline">\(\sqrt{d_{model}}\)</span>, donde <span class="math inline">\(d_{model}\)</span> es el tamaño del hidden state, <code>hid_dim</code>. Esto supuestamente reduce la variación en las incorporaciones y el modelo es difícil de entrenar de manera confiable sin este factor de escala. A continuación, se aplica el dropout a las embeddings combinadas.</p>
<p>Las embedding combinadas luego se pasan a través de <span class="math inline">\(N\)</span> <em>capas de encoder</em> para obtener <span class="math inline">\(Z\)</span>, que luego se van de output y puede ser utilizado por el decoder.</p>
<p>La máscara fuente, <code>src_mask</code>, tiene simplemente la misma forma que la oración fuente pero tiene un valor de 1 cuando el token en la oración fuente no es un token <code>&lt;pad&gt;</code> y 0 cuando es un <code>&lt;pad&gt;</code>. simbólico. Esto se usa en las capas del encoder para enmascarar los mecanismos de atención de múltiples cabezas, que se usan para calcular y aplicar atención sobre la oración fuente, por lo que el modelo no presta atención a los tokens <code>&lt;pad&gt;</code>, que no contienen información útil.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:42.074484Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:42.060171Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;3bfb25f3ec47b4d758a8136abc9f7c67&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-b020906aa22bcf9d&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="17">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>                 input_dim, </span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>                 hid_dim, </span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>                 n_layers, </span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>                 n_heads, </span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>                 pf_dim,</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>                 dropout, </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>                 device,</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>                 max_length <span class="op">=</span> <span class="dv">100</span>):</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 2 lineas para</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.tok_embedding =</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.pos_embedding =</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tok_embedding <span class="op">=</span> nn.Embedding(input_dim, hid_dim)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embedding <span class="op">=</span> nn.Embedding(max_length, hid_dim)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([EncoderLayer(hid_dim, </span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>                                                  n_heads, </span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>                                                  pf_dim,</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>                                                  dropout, </span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>                                                  device) </span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>                                     <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layers)])</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.dropout =</span></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Use el valor para dropout dado en la firma del constructor</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> torch.sqrt(torch.FloatTensor([hid_dim])).to(device)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, src_mask):</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Noten que el src y el src_mask son lista con informacion dentro de ellas</span></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src = [batch size, src len]</span></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src_mask = [batch size, 1, 1, src len]</span></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 2 lineas para</span></span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_size =</span></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># src_len = </span></span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> src.shape[<span class="dv">0</span>]</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>        src_len <span class="op">=</span> src.shape[<span class="dv">1</span>]</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>        pos <span class="op">=</span> torch.arange(<span class="dv">0</span>, src_len).unsqueeze(<span class="dv">0</span>).repeat(batch_size, <span class="dv">1</span>).to(<span class="va">self</span>.device)</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Noten que pos tendra informacion del batch y el tamanio del src</span></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pos = [batch size, src len]</span></span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>        src <span class="op">=</span> <span class="va">self</span>.dropout((<span class="va">self</span>.tok_embedding(src) <span class="op">*</span> <span class="va">self</span>.scale) <span class="op">+</span> <span class="va">self</span>.pos_embedding(pos))</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># src = [batch size, src len, hid dim]</span></span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>            src <span class="op">=</span> layer(src, src_mask)</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># src = [batch size, src len, hid dim]</span></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> src</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="capa-de-encoder" class="level3">
<h3 class="anchored" data-anchor-id="capa-de-encoder">Capa de Encoder</h3>
<p>Las capas del encoder son donde está contenida toda la “carne” del codificador. Primero pasamos la oración fuente y su máscara a la <em>capa de atención de múltiples cabezas</em>, luego realizamos el dropout, aplicamos una conexión residual y la pasamos a través de una <a href="https://arxiv.org/abs/1607.06450">Normalización de capa</a>. Luego lo pasamos a través de una capa de <em>position-wise feedforward</em> y luego, nuevamente, aplicamos dropout, una conexión residual y luego la normalización de la capa para obtener la salida de esta capa que se alimenta a la siguiente capa. Los parámetros no se comparten entre capas.</p>
<p>La capa encoder utiliza la capa de atención de múltiples cabezas para prestar atención a la oración fuente, es decir, está calculando y aplicando atención sobre sí misma en lugar de sobre otra secuencia, por lo que la llamamos <em>autoatención</em>.</p>
<p><a href="https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/">Este</a> artículo entra en más detalles sobre la capa normalización, pero la esencia es que normaliza los valores de las features, es decir, a través de la hidden dimension, por lo que cada característica tiene una media de 0 y una desviación estándar de 1. Esto permite a las redes neuronales con una mayor cantidad de capas, como el Transformador , el poder entrear más fácil.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:42.089677Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:42.076296Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;05ab897605499925278d5b00a2712bdf&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-227fb230ab0a08a6&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="18">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderLayer(nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>                 hid_dim, </span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>                 n_heads, </span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>                 pf_dim,  </span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>                 dropout, </span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>                 device):</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 2 lineas para</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.self_attn_layer_norm =</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.ff_layer_norm =</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn_layer_norm <span class="op">=</span> nn.LayerNorm(hid_dim)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_layer_norm <span class="op">=</span> nn.LayerNorm(hid_dim)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attention <span class="op">=</span> MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positionwise_feedforward <span class="op">=</span> PositionwiseFeedforwardLayer(hid_dim, </span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>                                                                     pf_dim, </span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>                                                                     dropout)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, src_mask):</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src = [batch size, src len, hid dim]</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src_mask = [batch size, 1, 1, src len] </span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 lineas para self attention</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># _src, _ =</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>        _src, _ <span class="op">=</span> <span class="va">self</span>.self_attention(src, src, src, src_mask)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">#dropout, residual connection y layer norm</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        src <span class="op">=</span> <span class="va">self</span>.self_attn_layer_norm(src <span class="op">+</span> <span class="va">self</span>.dropout(_src))</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src = [batch size, src len, hid dim]</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">#positionwise feedforward</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        _src <span class="op">=</span> <span class="va">self</span>.positionwise_feedforward(src)</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">#dropout, residual and layer norm</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>        src <span class="op">=</span> <span class="va">self</span>.ff_layer_norm(src <span class="op">+</span> <span class="va">self</span>.dropout(_src))</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src = [batch size, src len, hid dim]</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> src</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="multi-head-attention-layer" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-attention-layer">Multi Head Attention Layer</h3>
<p>Uno de los conceptos clave y novedosos introducidos por el artículo de Transformer es la <em>capa de atención de múltiples cabezas</em>.</p>
<p><img src="https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/49df8404d938a6edbf729876405558cc2c2b3013//assets/transformer-attention.png" class="img-fluid"></p>
<p>La atención se puede considerar como <em>querys</em>, <em>keys</em> y <em>values</em>, donde la query se usa con la key para obtener un vector de atención (generalmente el resultado de una operación <em>softmax</em> y tiene todos los values entre 0 y 1 que suma a 1) que luego se usa para obtener una suma ponderada de los values.</p>
<p>El transformador utiliza <em>atención de producto escalar “escalado”</em>, donde la query y la key se combinan tomando el producto escalar entre ellos, luego aplicando la operación softmax y escalando por <span class="math inline">\(d_k\)</span> antes de finalmente multiplicar por el value. <span class="math inline">\(d_k\)</span> que es la <em>dimensión de la cabeza</em>, <code>head_dim</code>, que explicaremos más adelante.</p>
<p><span class="math display">\[ \text{Atención}(Q, K, V) = \text{Softmax} \big( \frac{QK^T}{\sqrt{d_k}} \big)V \]</span></p>
<p>Esto es similar a la <em>atención estándar del producto escalar</em> pero se escala por <span class="math inline">\(d_k\)</span>, que según el documento se usa para evitar que los resultados de los productos escalares crezcan demasiado, lo que hace que los gradientes se vuelvan demasiado pequeños.</p>
<p>Sin embargo, la atención del producto punto escalado no se aplica simplemente a las querys, keys y values. En lugar de hacer una sola aplicación de atención, las querys, las keys y los valuees tienen su <code>hid_dim</code> dividido en <span class="math inline">\(h\)</span> <em>cabezas</em> y la atención del producto punto escalado se calcula sobre todas las cabezas en paralelo. Esto significa que en lugar de prestar atención a un concepto por aplicación de atención, prestamos atención a <span class="math inline">\(h\)</span>. Luego, volvemos a combinar las cabezas en su forma <code>hid_dim</code>, por lo que cada <code>hid_dim</code> está potencialmente prestando atención a <span class="math inline">\(h\)</span> conceptos diferentes.</p>
<p><span class="math display">\[ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1,...,\text{head}_h)W^O \]</span></p>
<p><span class="math display">\[\text{cabeza}_i = \text{Atención}(QW_i^Q, KW_i^K, VW_i^V) \]</span></p>
<p><span class="math inline">\(W^O\)</span> es la capa lineal aplicada al final de la capa de atención de múltiples cabezas, <code>fc</code>. <span class="math inline">\(W^Q, W^K, W^V\)</span> son las capas lineales <code>fc_q</code>, <code>fc_k</code> y <code>fc_v</code>.</p>
<p>Recorriendo el módulo, primero calculamos <span class="math inline">\(QW^Q\)</span>, <span class="math inline">\(KW^K\)</span> y <span class="math inline">\(VW^V\)</span> con las capas lineales, <code>fc_q</code>, <code>fc_k</code> y <code>fc_v</code>, para obtener <code>Q</code>, <code>K</code> y <code>V</code>. A continuación, dividimos <code>hid_dim</code> de la query, la key y el value en <code>n_heads</code> usando <code>.view</code> y los permutamos correctamente para que puedan multiplicarse entre sí. Luego calculamos la ‘energía’ (la atención no normalizada) multiplicando ‘Q’ y ‘K’ juntos y escalando por la raíz cuadrada de ‘head_dim’, que se calcula como ‘hid_dim // n_heads’. Luego enmascaramos la energía para que no prestemos atención a ningún elemento de la secuencia que no deberíamos, luego aplicamos el softmax y el dropout. Luego aplicamos la atención a los values caras, <code>V</code>, antes de combinar los <code>n_cabezas</code>. Finalmente, multiplicamos este <span class="math inline">\(W^O\)</span>, representado por <code>fc_o</code>.</p>
<p>Note que en nuestra implementación, las longitudes de las keys y los values son siempre los mismos, por lo tanto, cuando la matriz multiplica la salida del softmax, <code>atención</code>, con <code>V</code>, siempre tendremos tamaños de dimensión válidos para la multiplicación de matrices. Esta multiplicación se lleva a cabo usando <code>torch.matmul</code> que, cuando ambos tensores son &gt; bidimensionales, realiza una multiplicación matricial por batches sobre las dos últimas dimensiones de cada tensor. Esta será una <strong>[longitud de query, longitud de key] x [longitud de value, atenuación de cabezal]</strong> multiplicación de matriz por batches sobre el tamaño del batch y cada cabezal que proporciona el <strong>[tamaño de batch, n cabezales, longitud de query, atenuación de cabezal ]</strong> resultado.</p>
<p>Una cosa que parece extraña al principio es que dropout se aplica directamente a la atención. Esto significa que nuestro vector de atención probablemente no sumará 1 y podemos prestar toda la atención a un token, pero la atención sobre ese token se establece en 0 por dropout. Esto nunca se explica, ni siquiera se menciona, en el documento; sin embargo, lo usa la <a href="https://github.com/tensorflow/tensor2tensor/">implementación oficial</a> y todas las implementaciones de Transformer desde [BERT] (https:// github.com/google-research/bert/).</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:42.105676Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:42.092711Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;5eff2884d202ca050348224f531628c8&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-7d125b83ae6200ef&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="19">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttentionLayer(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hid_dim, n_heads, dropout, device):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> hid_dim <span class="op">%</span> n_heads <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hid_dim <span class="op">=</span> hid_dim</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_heads <span class="op">=</span> n_heads</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> hid_dim <span class="op">//</span> n_heads</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 4 lineas para</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.fc_q = </span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.fc_k =</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.fc_v =</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.fc_o =</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Probablemente necesite nn.Linear</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_q <span class="op">=</span> nn.Linear(hid_dim, hid_dim)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_k <span class="op">=</span> nn.Linear(hid_dim, hid_dim)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_v <span class="op">=</span> nn.Linear(hid_dim, hid_dim)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_o <span class="op">=</span> nn.Linear(hid_dim, hid_dim)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> torch.sqrt(torch.FloatTensor([<span class="va">self</span>.head_dim])).to(device)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> query.shape[<span class="dv">0</span>]</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">#query = [batch size, query len, hid dim]</span></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">#key = [batch size, key len, hid dim]</span></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">#value = [batch size, value len, hid dim]</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.fc_q(query)</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.fc_k(key)</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.fc_v(value)</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">#Q = [batch size, query len, hid dim]</span></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">#K = [batch size, key len, hid dim]</span></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">#V = [batch size, value len, hid dim]</span></span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> Q.view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.n_heads, <span class="va">self</span>.head_dim).permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aproximadamente 2 lineas para</span></span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># K =</span></span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># V =</span></span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Probablemente necesite el metodo .view y .permute</span></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> K.view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.n_heads, <span class="va">self</span>.head_dim).permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> V.view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.n_heads, <span class="va">self</span>.head_dim).permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>        <span class="co">#Q = [batch size, n heads, query len, head dim]</span></span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>        <span class="co">#K = [batch size, n heads, key len, head dim]</span></span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>        <span class="co">#V = [batch size, n heads, value len, head dim]</span></span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>        energy <span class="op">=</span> torch.matmul(Q, K.permute(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)) <span class="op">/</span> <span class="va">self</span>.scale</span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>        <span class="co">#energy = [batch size, n heads, query len, key len]</span></span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>            energy <span class="op">=</span> energy.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span><span class="fl">1e10</span>)</span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a>        attention <span class="op">=</span> torch.softmax(energy, dim <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb21-64"><a href="#cb21-64" aria-hidden="true" tabindex="-1"></a>        <span class="co">#attention = [batch size, n heads, query len, key len]</span></span>
<span id="cb21-65"><a href="#cb21-65" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb21-66"><a href="#cb21-66" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.matmul(<span class="va">self</span>.dropout(attention), V)</span>
<span id="cb21-67"><a href="#cb21-67" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-68"><a href="#cb21-68" aria-hidden="true" tabindex="-1"></a>        <span class="co">#x = [batch size, n heads, query len, head dim]</span></span>
<span id="cb21-69"><a href="#cb21-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-70"><a href="#cb21-70" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>).contiguous()</span>
<span id="cb21-71"><a href="#cb21-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-72"><a href="#cb21-72" aria-hidden="true" tabindex="-1"></a>        <span class="co">#x = [batch size, query len, n heads, head dim]</span></span>
<span id="cb21-73"><a href="#cb21-73" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-74"><a href="#cb21-74" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.hid_dim)</span>
<span id="cb21-75"><a href="#cb21-75" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-76"><a href="#cb21-76" aria-hidden="true" tabindex="-1"></a>        <span class="co">#x = [batch size, query len, hid dim]</span></span>
<span id="cb21-77"><a href="#cb21-77" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-78"><a href="#cb21-78" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc_o(x)</span>
<span id="cb21-79"><a href="#cb21-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-80"><a href="#cb21-80" aria-hidden="true" tabindex="-1"></a>        <span class="co">#x = [batch size, query len, hid dim]</span></span>
<span id="cb21-81"><a href="#cb21-81" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-82"><a href="#cb21-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, attention</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="capa-position-wise-feedforward" class="level3">
<h3 class="anchored" data-anchor-id="capa-position-wise-feedforward">Capa Position-wise Feedforward</h3>
<p>El otro bloque principal dentro de la capa del encoder es la <em>capa de realimentación por posición</em> o <em>capa position-wise feedforward</em>. Es relativamente simple en comparación con la capa de atención multi-head. La entrada se transforma de <code>hid_dim</code> a <code>pf_dim</code>, donde <code>pf_dim</code> suele ser mucho más grande que <code>hid_dim</code>. El Transformer original usaba un <code>hid_dim</code> de 512 y un <code>pf_dim</code> de 2048. La función de activación y dropout de ReLU se aplica antes de que se transforme de nuevo en una representación <code>hid_dim</code>.</p>
<p>¿Por qué se usa esto? Desafortunadamente, nunca se explica en el documento.</p>
<p>BERT usa la función de activación <a href="https://arxiv.org/abs/1606.08415">GELU</a>, que se puede usar simplemente cambiando <code>torch.relu</code> por <code>F.gelu</code>. ¿Por qué usaron GELU? De nuevo, lastimosamente, no se explica.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:42.120748Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:42.106674Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;cd68e30d64c61b52c3cdf3850a9acb54&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-b0852a606228b06c&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="20">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionwiseFeedforwardLayer(nn.Module):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hid_dim, pf_dim, dropout):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 2 lineas para</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.fc_1 = </span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.fc_2 = </span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: hid_dim y pf_dim</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_1 <span class="op">=</span> nn.Linear(hid_dim, pf_dim)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_2 <span class="op">=</span> nn.Linear(pf_dim, hid_dim)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">#x = [batch size, seq len, hid dim]</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(torch.relu(<span class="va">self</span>.fc_1(x)))</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">#x = [batch size, seq len, pf dim]</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc_2(x)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">#x = [batch size, seq len, hid dim]</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="decoder" class="level3">
<h3 class="anchored" data-anchor-id="decoder">Decoder</h3>
<p>El objetivo del decoder es tomar la representación codificada de la oración de origen, <span class="math inline">\(Z\)</span>, y convertirla en tokens predichos en la oración de destino, <span class="math inline">\(\hat{Y}\)</span>. Luego comparamos <span class="math inline">\(\hat{Y}\)</span> con los tokens reales en la oración objetivo, <span class="math inline">\(Y\)</span>, para calcular nuestra pérdida, que se usará para calcular los gradientes de nuestros parámetros y luego usamos nuestro optimizador para actualizar nuestros pesos en orden para mejorar nuestras predicciones.</p>
<p><img src="https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/49df8404d938a6edbf729876405558cc2c2b3013//assets/transformer-decoder.png" class="img-fluid"></p>
<p>El decoder es similar al encoder, sin embargo, ahora tiene dos capas de atención multi-head. Una <em>capa de atención multi-head enmascarada</em> sobre la secuencia de destino y una capa de atención multi-head que utiliza la representación del decoder como consulta y la representación del encoder como clave y valor.</p>
<p>El decoder utiliza positional embeddings y las combina, a través de una suma de elementos, con los tokens de destino embeddidos escalados, seguidos de dropout. Nuevamente, nuestras codificaciones posicionales tienen un “vocabulario” de 100, lo que significa que pueden aceptar secuencias de hasta 100 tokens de largo. Esto se puede aumentar si se desea.</p>
<p>Las embeddings combinadas luego se pasan a través de las capas del decodificador <span class="math inline">\(N\)</span>, junto con la fuente codificada, <code>enc_src</code>, y las máscaras de origen y destino. Considere que la cantidad de capas en el encoder no tiene que ser igual a la cantidad de capas en el decoder, aunque ambas se indican con <span class="math inline">\(N\)</span>.</p>
<p>La representación del decoder después de la capa <span class="math inline">\(N^{th}\)</span> se pasa a través de una capa lineal, <code>fc_out</code>. En PyTorch, la operación softmax está contenida dentro de nuestra función de pérdida, por lo que no necesitamos usar explícitamente una capa softmax aquí.</p>
<p>Además de usar la máscara de origen, como hicimos en el encoder para evitar que nuestro modelo preste atención a los tokens <code>&lt;pad&gt;</code>, también usamos una máscara de destino. Esto se explicará con más detalle en el modelo <code>Seq2Seq</code> que encapsula tanto el encoder como el decoder. Como estamos procesando todos los tokens de destino a la vez en paralelo, necesitamos un método para evitar que el decoder “haga trampa” simplemente “mirando” cuál es el siguiente token en la secuencia de destino y emitiéndolo.</p>
<p>Nuestra capa de decoder también genera los valores de atención normalizados para que luego podamos trazarlos y ver a qué está prestando atención nuestro modelo.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:42.136660Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:42.121698Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;af87a789f88f1a0278c83c566e4eec14&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-aaffd5baa47612d2&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="21">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Decoder(nn.Module):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>                 output_dim, </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>                 hid_dim, </span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>                 n_layers, </span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>                 n_heads, </span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>                 pf_dim, </span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>                 dropout, </span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>                 device,</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>                 max_length <span class="op">=</span> <span class="dv">100</span>):</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 2 lineas para</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.tok_embedding =</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.pos_embedding = </span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: output_dim y hid_dim</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tok_embedding <span class="op">=</span> nn.Embedding(output_dim, hid_dim)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embedding <span class="op">=</span> nn.Embedding(max_length, hid_dim)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList()</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layers):</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Aprox 1 linea para </span></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># layer = </span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Hint: DecoderLayer</span></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># YOUR CODE HERE</span></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.layers.append(layer)</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_out <span class="op">=</span> nn.Linear(hid_dim, output_dim)</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> torch.sqrt(torch.FloatTensor([hid_dim])).to(device)</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, trg, enc_src, trg_mask, src_mask):</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [batch size, trg len]</span></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">#enc_src = [batch size, src len, hid dim]</span></span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg_mask = [batch size, 1, trg len, trg len]</span></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src_mask = [batch size, 1, 1, src len]</span></span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> trg.shape[<span class="dv">0</span>]</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>        trg_len <span class="op">=</span> trg.shape[<span class="dv">1</span>]</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>        pos <span class="op">=</span> torch.arange(<span class="dv">0</span>, trg_len).unsqueeze(<span class="dv">0</span>).repeat(batch_size, <span class="dv">1</span>).to(<span class="va">self</span>.device)</span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>                            </span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>        <span class="co">#pos = [batch size, trg len]</span></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>        trg <span class="op">=</span> <span class="va">self</span>.dropout((<span class="va">self</span>.tok_embedding(trg) <span class="op">*</span> <span class="va">self</span>.scale) <span class="op">+</span> <span class="va">self</span>.pos_embedding(pos))</span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [batch size, trg len, hid dim]</span></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Aprox 1 linea para</span></span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a>            <span class="co"># trg, attetiont = </span></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Hint: use layer(...)</span></span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a>            <span class="co"># YOUR CODE HERE</span></span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a>            trg, attention <span class="op">=</span> layer(trg, enc_src, trg_mask, src_mask)</span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [batch size, trg len, hid dim]</span></span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a>        <span class="co">#attention = [batch size, n heads, trg len, src len]</span></span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.fc_out(trg)</span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a>        <span class="co">#output = [batch size, trg len, output dim]</span></span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="decoder-layer" class="level3">
<h3 class="anchored" data-anchor-id="decoder-layer">Decoder Layer</h3>
<p>Como se mencionó antes, la capa del decoder es similar a la capa del encoder, excepto que ahora tiene dos capas de atención multi-head, <code>self_attention</code> y <code>encoder_attention</code>.</p>
<p>El primero realiza la autoatención, como en el encoder, utilizando la representación del decoder en cuanto a query, key y value. A esto le sigue el dropout, la conexión residual y la normalización de capas. Esta capa <code>self_attention</code> utiliza la máscara de secuencia de destino, <code>trg_mask</code>, para evitar que el decoder “haga trampa” al prestar atención a los tokens que están “por delante” del que está procesando actualmente, ya que procesa todos los tokens en el objetivo. oración en paralelo.</p>
<p>El segundo es cómo alimentamos la oración fuente codificada, <code>enc_src</code>, en nuestro decoder. En esta capa de atención de multi-head, las queries son las representaciones del decoder y las keys y los values son las representaciones del encoder. Aquí, la máscara de origen, <code>src_mask</code> se usa para evitar que la capa de atención multi-head preste atención a los tokens <code>&lt;pad&gt;</code> dentro de la oración de origen. A esto le siguen las capas de dropout, conexión residual y normalización de capas.</p>
<p>Finalmente, pasamos esto a través de la capa de position-wise feedforward y otra secuencia más de dropout, conexión residual y normalización de capa.</p>
<p>La capa del decoder no presenta ningún concepto nuevo, solo usa el mismo conjunto de capas que el encoder de una manera ligeramente diferente.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:42.152305Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:42.138655Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;93076e990b861c9fd91f3595afd1b3db&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-ea18b5a4e82e539c&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="22">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecoderLayer(nn.Module):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>                 hid_dim, </span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>                 n_heads, </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>                 pf_dim, </span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>                 dropout, </span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>                 device):</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 3 lineas para</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.self_attn_layer_norm =</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.enc_attn_layer_norm =</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.ff_layer_norm = </span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn_layer_norm <span class="op">=</span> nn.LayerNorm(hid_dim)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc_attn_layer_norm <span class="op">=</span> nn.LayerNorm(hid_dim)</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_layer_norm <span class="op">=</span> nn.LayerNorm(hid_dim)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attention <span class="op">=</span> MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_attention <span class="op">=</span> MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positionwise_feedforward <span class="op">=</span> PositionwiseFeedforwardLayer(hid_dim, </span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>                                                                     pf_dim, </span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>                                                                     dropout)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, trg, enc_src, trg_mask, src_mask):</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [batch size, trg len, hid dim]</span></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">#enc_src = [batch size, src len, hid dim]</span></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg_mask = [batch size, 1, trg len, trg len]</span></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src_mask = [batch size, 1, 1, src len]</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">#self attention</span></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>        _trg, _ <span class="op">=</span> <span class="va">self</span>.self_attention(trg, trg, trg, trg_mask)</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">#dropout, residual connection and layer norm</span></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>        trg <span class="op">=</span> <span class="va">self</span>.self_attn_layer_norm(trg <span class="op">+</span> <span class="va">self</span>.dropout(_trg))</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [batch size, trg len, hid dim]</span></span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">#encoder attention</span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>        _trg, attention <span class="op">=</span> <span class="va">self</span>.encoder_attention(trg, enc_src, enc_src, src_mask)</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">#dropout, residual connection and layer norm</span></span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>        trg <span class="op">=</span> <span class="va">self</span>.enc_attn_layer_norm(trg <span class="op">+</span> <span class="va">self</span>.dropout(_trg))</span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [batch size, trg len, hid dim]</span></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>        <span class="co">#positionwise feedforward</span></span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>        _trg <span class="op">=</span> <span class="va">self</span>.positionwise_feedforward(trg)</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>        <span class="co">#dropout, residual and layer norm</span></span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>        trg <span class="op">=</span> <span class="va">self</span>.ff_layer_norm(trg <span class="op">+</span> <span class="va">self</span>.dropout(_trg))</span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [batch size, trg len, hid dim]</span></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>        <span class="co">#attention = [batch size, n heads, trg len, src len]</span></span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> trg, attention</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="modelo-seq2seq" class="level3">
<h3 class="anchored" data-anchor-id="modelo-seq2seq">Modelo Seq2Seq</h3>
<p>Finalmente, tenemos el módulo <code>Seq2Seq</code> que encapsula el encoder y decoder, además de manejar la creación de las máscaras.</p>
<p>La máscara de origen se crea comprobando dónde la secuencia de origen no es igual a un token <code>&lt;pad&gt;</code>. Es 1 cuando el token no es un token <code>&lt;pad&gt;</code> y 0 cuando lo es. Luego se descomprime para que pueda transmitirse correctamente al aplicar la máscara a la <code>energía</code>, que tiene la forma <strong><em>[tamaño del batch, n cabezas, seq len, seq len]</em></strong>.</p>
<p>La máscara de destino es un poco más complicada. Primero, creamos una máscara para los tokens <code>&lt;pad&gt;</code>, como hicimos con la máscara fuente. A continuación, creamos una máscara “subsecuente”, <code>trg_sub_mask</code>, usando <code>torch.tril</code>. Esto crea una matriz diagonal donde los elementos por encima de la diagonal serán cero y los elementos por debajo de la diagonal se establecerán en cualquiera que sea el tensor de entrada. En este caso, el tensor de entrada será un tensor lleno de unos. Esto significa que nuestra <code>trg_sub_mask</code> se verá así (para un objetivo con 5 tokens):</p>
<p><span class="math display">\[
1  0  0  0  0\\
1  1  0  0  0\\
1  1  1  0  0\\
1  1  1  1  0\\
1  1  1  1  1\\
\]</span></p>
<p>Esto muestra lo que cada token de destino (fila) puede ver (columna). El primer token de destino tiene una máscara de <strong><em>[1, 0, 0, 0, 0]</em></strong>, lo que significa que solo puede mirar el primer token de destino. El segundo token de destino tiene una máscara de <strong><em>[1, 1, 0, 0, 0]</em></strong>, lo que significa que puede ver tanto la primera como la segunda ficha de destino.</p>
<p>A continuación, la máscara “subsecuente” se combina lógicamente con la máscara de relleno, lo que combina las dos máscaras, lo que garantiza que no se pueda atender ni a los tokens posteriores ni a los tokens de relleno. Por ejemplo, si los dos últimos tokens fueran tokens <code>&lt;pad&gt;</code>, la máscara se vería así:</p>
<p><span class="math display">\[
1  0  0  0  0\\
1  1  0  0  0\\
1  1  1  0  0\\
1  1  1  0  0\\
1  1  1  0  0\\
\]</span></p>
<p>Después de crear las máscaras, se utilizan con el encoder y el decoder junto con las oraciones de origen y de destino para obtener nuestra oración de destino predicha, “salida”, junto con la atención del decoder sobre la secuencia de origen.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:42.168209Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:42.154164Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;1dfbdb7f7fe10f8e3be20088952eb5ec&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-1532564591007a69&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="23">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Seq2Seq(nn.Module):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>                 encoder, </span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>                 decoder, </span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>                 src_pad_idx, </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>                 trg_pad_idx, </span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>                 device):</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 5 lineas para</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.encoder = </span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.decoder = </span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.src_pad_idx = </span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.trg_pad_idx = </span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.device = </span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_pad_idx <span class="op">=</span> src_pad_idx</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.trg_pad_idx <span class="op">=</span> trg_pad_idx</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> make_src_mask(<span class="va">self</span>, src):</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src = [batch size, src len]</span></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>        src_mask <span class="op">=</span> (src <span class="op">!=</span> <span class="va">self</span>.src_pad_idx).unsqueeze(<span class="dv">1</span>).unsqueeze(<span class="dv">2</span>)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src_mask = [batch size, 1, 1, src len]</span></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> src_mask</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> make_trg_mask(<span class="va">self</span>, trg):</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [batch size, trg len]</span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>        trg_pad_mask <span class="op">=</span> (trg <span class="op">!=</span> <span class="va">self</span>.trg_pad_idx).unsqueeze(<span class="dv">1</span>).unsqueeze(<span class="dv">2</span>)</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg_pad_mask = [batch size, 1, 1, trg len]</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>        trg_len <span class="op">=</span> trg.shape[<span class="dv">1</span>]</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>        trg_sub_mask <span class="op">=</span> torch.tril(torch.ones((trg_len, trg_len), device <span class="op">=</span> <span class="va">self</span>.device)).<span class="bu">bool</span>()</span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg_sub_mask = [trg len, trg len]</span></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>        trg_mask <span class="op">=</span> trg_pad_mask <span class="op">&amp;</span> trg_sub_mask</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg_mask = [batch size, 1, trg len, trg len]</span></span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> trg_mask</span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, trg):</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src = [batch size, src len]</span></span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [batch size, trg len]</span></span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>        src_mask <span class="op">=</span> <span class="va">self</span>.make_src_mask(src)</span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a>        trg_mask <span class="op">=</span> <span class="va">self</span>.make_trg_mask(trg)</span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src_mask = [batch size, 1, 1, src len]</span></span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg_mask = [batch size, 1, trg len, trg len]</span></span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a>        enc_src <span class="op">=</span> <span class="va">self</span>.encoder(src, src_mask)</span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>        <span class="co">#enc_src = [batch size, src len, hid dim]</span></span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>        output, attention <span class="op">=</span> <span class="va">self</span>.decoder(trg, enc_src, trg_mask, src_mask)</span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a>        <span class="co">#output = [batch size, trg len, output dim]</span></span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a>        <span class="co">#attention = [batch size, n heads, trg len, src len]</span></span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="entrenamiento" class="level3">
<h3 class="anchored" data-anchor-id="entrenamiento">Entrenamiento</h3>
<p>Ahora ya podemos entrenar nuestro modelo, el cual es más pequeño que el modelo usado en el paper original, pero es lo suficientemente robusto.</p>
<p>Luego, vamos a definir nuestro modelo completo sequence-to-sequence.</p>
<p>Después, creamos una función para contar el número de parámetros, notando que esta vez ya estamos hablando de millones de parametros dentro de un modelo.</p>
<p>Más tarde, definimos la forma de iniciar los pesos, usando una técnica conocida como Xavier uniform.</p>
<p>Luego, el optimizador utilizado con un learning rate fijo es declarado. Consideren que el learning rate debe ser inferior a la predeterminada utilizada por Adam o, de lo contrario, el aprendizaje es inestable.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:43.519063Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:42.170172Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;39d8fe528427f96645ebc4c356b99f83&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-4d4df7d3c94c21d7&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="24">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>INPUT_DIM <span class="op">=</span> <span class="bu">len</span>(SRC.vocab)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>OUTPUT_DIM <span class="op">=</span> <span class="bu">len</span>(TRG.vocab)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>HID_DIM <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>ENC_LAYERS <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>DEC_LAYERS <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>ENC_HEADS <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>DEC_HEADS <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>ENC_PF_DIM <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>DEC_PF_DIM <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>ENC_DROPOUT <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>DEC_DROPOUT <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>enc <span class="op">=</span> Encoder(INPUT_DIM, </span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>              HID_DIM, </span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>              ENC_LAYERS, </span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>              ENC_HEADS, </span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>              ENC_PF_DIM, </span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>              ENC_DROPOUT, </span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>              device)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>dec <span class="op">=</span> Decoder(OUTPUT_DIM, </span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>              HID_DIM, </span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>              DEC_LAYERS, </span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>              DEC_HEADS, </span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>              DEC_PF_DIM, </span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>              DEC_DROPOUT, </span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>              device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:43.549933Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:43.521009Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;e650f8882f5eec23b28f11a76c13b51c&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-fd38ea080f9443e0&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="25">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>SRC_PAD_IDX <span class="op">=</span> SRC.vocab.stoi[SRC.pad_token]</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>TRG_PAD_IDX <span class="op">=</span> TRG.vocab.stoi[TRG.pad_token]</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:43.565869Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:43.550966Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;3b565b66a977c9daf53037715d6ce63e&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-bc0eeb460e049896&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="26">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_parameters(model):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'The model has </span><span class="sc">{</span>count_parameters(model)<span class="sc">:,}</span><span class="ss"> trainable parameters'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The model has 9,038,341 trainable parameters</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:43.581032Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:43.566787Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;3156cf3c171d68ad7e69c49e0eb63e62&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-4c249e5a1b31389d&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="27">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> initialize_weights(m):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">hasattr</span>(m, <span class="st">'weight'</span>) <span class="kw">and</span> m.weight.dim() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>        nn.init.xavier_uniform_(m.weight.data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:43.674671Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:43.584178Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;17fc578fcbdf63c5e6690b4ef4b8b882&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-ee40ffc46a2d900e&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="28">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">apply</span>(initialize_weights)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:43.690637Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:43.676665Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;a3db79a16ac561596f55e0a287d3850d&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-892680cab08efdd9&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="29">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">0.0005</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr <span class="op">=</span> LEARNING_RATE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:43.705588Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:43.692622Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;156ae3448b7c22a092c6f1759ec01f08&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-e8d5e65d680a62d2&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="30">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss(ignore_index <span class="op">=</span> TRG_PAD_IDX)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Como queremos que nuestro modelo prediga el token <code>&lt;eos&gt;</code> pero no que sea una entrada en nuestro modelo, simplemente cortamos el token <code>&lt;eos&gt;</code> del final de la secuencia. De este modo:</p>
<p><span class="math display">\[ \text{Atención}(Q, K, V) = \text{Softmax} \big( \frac{QK^T}{\sqrt{d_k}} \big)V \]</span></p>
<p><span class="math display">\[
\text{trg} = [sos, x_1, x_2, x_3, eos]\\
\text{trg[:-1]} = [sos, x_1, x_2, x_3]
\]</span></p>
<p><span class="math inline">\(x_i\)</span> denota el elemento de secuencia de destino real. Luego ingresamos esto en el modelo para obtener una secuencia predicha que debería predecir el token <code>&lt;eos&gt;</code>:</p>
<p><span class="math display">\[
\text{salida} = [y_1, y_2, y_3, eos]
\]</span></p>
<p><span class="math inline">\(y_i\)</span> denota el elemento de secuencia de destino predicho. Luego calculamos nuestra pérdida usando el tensor <code>trg</code> original con el token <code>&lt;sos&gt;</code> cortado del frente, dejando el token <code>&lt;eos&gt;</code>:</p>
<p><span class="math display">\[
\text{salida} = [y_1, y_2, y_3, eos]\\
\text{trg[1:]} = [x_1, x_2, x_3, eos]
\]</span></p>
<p>Luego calculamos nuestras losses y actualizamos nuestros parámetros como es estándar.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:43.721662Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:43.707582Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;2621fa6d9a664326a015b72467ac35ef&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-94a0f0c13a74c89b&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="31">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, iterator, optimizer, criterion, clip):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(iterator):</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        src <span class="op">=</span> batch.src</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        trg <span class="op">=</span> batch.trg</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        output, _ <span class="op">=</span> model(src, trg[:,:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">#output = [batch size, trg len - 1, output dim]</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [batch size, trg len]</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        output_dim <span class="op">=</span> output.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para</span></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output =</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Considere usar.contiguos</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.contiguous().view(<span class="op">-</span><span class="dv">1</span>, output_dim)</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>        trg <span class="op">=</span> trg[:,<span class="dv">1</span>:].contiguous().view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">#output = [batch size * trg len - 1, output dim]</span></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [batch size * trg len - 1]</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, trg)</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">+=</span> loss.item()</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> epoch_loss <span class="op">/</span> <span class="bu">len</span>(iterator)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El ciclo de evaluación es el mismo que el del entrenamiento pero sin la parte de la graiente y la actualizacion de los parametros</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:43.737697Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:43.723657Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;1f4f445a1ab1d6baaf0b0d2c872c2114&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-14fb112cbf46928e&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="32">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(model, iterator, criterion):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(iterator):</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>            src <span class="op">=</span> batch.src</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>            trg <span class="op">=</span> batch.trg</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>            output, _ <span class="op">=</span> model(src, trg[:,:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>            <span class="co">#output = [batch size, trg len - 1, output dim]</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>            <span class="co">#trg = [batch size, trg len]</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>            output_dim <span class="op">=</span> output.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> output.contiguous().view(<span class="op">-</span><span class="dv">1</span>, output_dim)</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>            trg <span class="op">=</span> trg[:,<span class="dv">1</span>:].contiguous().view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>            <span class="co">#output = [batch size * trg len - 1, output dim]</span></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>            <span class="co">#trg = [batch size * trg len - 1]</span></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(output, trg)</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>            epoch_loss <span class="op">+=</span> loss.item()</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> epoch_loss <span class="op">/</span> <span class="bu">len</span>(iterator)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:37:43.753182Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:43.739693Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;0208092e910028b2b5cfba093c9e35f3&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-e913674f3c754c57&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="33">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> epoch_time(start_time, end_time):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    elapsed_time <span class="op">=</span> end_time <span class="op">-</span> start_time</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    elapsed_mins <span class="op">=</span> <span class="bu">int</span>(elapsed_time <span class="op">/</span> <span class="dv">60</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    elapsed_secs <span class="op">=</span> <span class="bu">int</span>(elapsed_time <span class="op">-</span> (elapsed_mins <span class="op">*</span> <span class="dv">60</span>))</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> elapsed_mins, elapsed_secs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:50.094914Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:37:43.754175Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;3ee3974b8bef1598781d150859ac4857&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-284a2ab136ece792&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="34">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Una linea para definir el numero de epocas</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="co"># N_EPOCHS =</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>CLIP <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>best_valid_loss <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(N_EPOCHS):</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> train(model, train_iterator, optimizer, criterion, CLIP)</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>    valid_loss <span class="op">=</span> evaluate(model, valid_iterator, criterion)</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.time()</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>    epoch_mins, epoch_secs <span class="op">=</span> epoch_time(start_time, end_time)</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> valid_loss <span class="op">&lt;</span> best_valid_loss:</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>        best_valid_loss <span class="op">=</span> valid_loss</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>        torch.save(model.state_dict(), <span class="st">'tut6-model.pt'</span>)</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch: </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:02}</span><span class="ss"> | Time: </span><span class="sc">{</span>epoch_mins<span class="sc">}</span><span class="ss">m </span><span class="sc">{</span>epoch_secs<span class="sc">}</span><span class="ss">s'</span>)</span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\t</span><span class="ss">Train Loss: </span><span class="sc">{</span>train_loss<span class="sc">:.3f}</span><span class="ss"> | Train PPL: </span><span class="sc">{</span>math<span class="sc">.</span>exp(train_loss)<span class="sc">:7.3f}</span><span class="ss">'</span>)</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\t</span><span class="ss"> Val. Loss: </span><span class="sc">{</span>valid_loss<span class="sc">:.3f}</span><span class="ss"> |  Val. PPL: </span><span class="sc">{</span>math<span class="sc">.</span>exp(valid_loss)<span class="sc">:7.3f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 01 | Time: 0m 43s
    Train Loss: 4.230 | Train PPL:  68.723
     Val. Loss: 3.028 |  Val. PPL:  20.655
Epoch: 02 | Time: 0m 42s
    Train Loss: 2.816 | Train PPL:  16.711
     Val. Loss: 2.300 |  Val. PPL:   9.976
Epoch: 03 | Time: 0m 42s
    Train Loss: 2.232 | Train PPL:   9.321
     Val. Loss: 1.983 |  Val. PPL:   7.262
Epoch: 04 | Time: 0m 42s
    Train Loss: 1.883 | Train PPL:   6.570
     Val. Loss: 1.802 |  Val. PPL:   6.060
Epoch: 05 | Time: 0m 42s
    Train Loss: 1.636 | Train PPL:   5.136
     Val. Loss: 1.699 |  Val. PPL:   5.467
Epoch: 06 | Time: 0m 43s
    Train Loss: 1.448 | Train PPL:   4.255
     Val. Loss: 1.659 |  Val. PPL:   5.255
Epoch: 07 | Time: 0m 44s
    Train Loss: 1.293 | Train PPL:   3.642
     Val. Loss: 1.627 |  Val. PPL:   5.089
Epoch: 08 | Time: 0m 44s
    Train Loss: 1.168 | Train PPL:   3.216
     Val. Loss: 1.612 |  Val. PPL:   5.013
Epoch: 09 | Time: 0m 45s
    Train Loss: 1.060 | Train PPL:   2.888
     Val. Loss: 1.627 |  Val. PPL:   5.090
Epoch: 10 | Time: 0m 45s
    Train Loss: 0.965 | Train PPL:   2.626
     Val. Loss: 1.632 |  Val. PPL:   5.114</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:50.266478Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:50.097847Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;d9c2cdf766f5e502e6484bd6aabb995b&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-861a3278292854db&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="35">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(torch.load(<span class="st">'tut6-model.pt'</span>))</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> evaluate(model, test_iterator, criterion)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'| Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">:.3f}</span><span class="ss"> | Test PPL: </span><span class="sc">{</span>math<span class="sc">.</span>exp(test_loss)<span class="sc">:7.3f}</span><span class="ss"> |'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>| Test Loss: 1.670 | Test PPL:   5.313 |</code></pre>
</div>
</div>
<p><strong>NB:</strong> La perplejidad (PPL) es una medida utilizada para evaluar la efectividad de un modelo de lenguaje al predecir una secuencia de palabras. Cuantifica qué tan bien el modelo predice la siguiente palabra en una secuencia basada en las palabras anteriores. Una perplejidad más baja indica que el modelo tiene más certeza y precisión en sus predicciones, lo que refleja una mejor comprensión del lenguaje. Por otro lado, una perplejidad más alta sugiere que el modelo tiene menos certeza y le cuesta predecir la siguiente palabra con precisión. La perplejidad se utiliza comúnmente en el procesamiento del lenguaje natural para evaluar la calidad de los modelos de lenguaje, especialmente en tareas como la traducción automática y la generación de texto.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:50.282549Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:50.268487Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;007cde74a1e6855f13d0ce41c80e3ff9&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-e75e31cfaa4d346a&quot;,&quot;locked&quot;:true,&quot;points&quot;:50,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="36">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">25</span>):        </span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_numbers(new_representation(test_loss), <span class="st">"3c3d"</span>, <span class="st">'0x1.ae147ae147ae1p+0'</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">25</span>):        </span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_numbers(new_representation(math.exp(test_loss)), <span class="st">"3c3d"</span>, <span class="st">'0x1.570a3d70a3d71p+2'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"25"}--> 
         ✓ [25 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"25"}--> 
         ✓ [25 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="inferencia" class="level3">
<h3 class="anchored" data-anchor-id="inferencia">Inferencia</h3>
<p>Ahora traduciremos desde nuestro modelo con la funcion dada abajo.</p>
<p>Los pasos tomados son: - Tokenizar la oración fuente si no ha sido tokenizada (es una cadena) - Agregar los tokens <code>&lt;sos&gt;</code> y <code>&lt;eos&gt;</code> - Numerizar la oración fuente - Convertirlo en un tensor y agregue una dimensión de lote - Crear la máscara de oración fuente - Introduce la oración fuente y la máscara en el codificador - Cree una lista para contener la oración de salida, inicializada con un token <code>&lt;sos&gt;</code> - Si bien no hemos alcanzado una longitud máxima - Convertir la predicción de la oración de salida actual en un tensor con una dimensión por lotes - Crear una máscara de oración objetivo - Coloque la salida actual, la salida del codificador y ambas máscaras en el decodificador - Obtenga la próxima predicción del token de salida del decodificador junto con la atención - Agregue predicción a la predicción de oración de salida actual - Interrumpir si la predicción fue un token <code>&lt;eos&gt;</code> - Convertir la oración de salida de índices a tokens - Devolver la oración de salida (con el token <code>&lt;sos&gt;</code> eliminado) y la atención de la última capa</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:50.297184Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:50.283549Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;8565ee3167fb563a2bf50a15ece1fa40&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-b5188a3de284fe92&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="37">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> translate_sentence(sentence, src_field, trg_field, model, device, max_len <span class="op">=</span> <span class="dv">50</span>):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(sentence, <span class="bu">str</span>):</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>        nlp <span class="op">=</span> spacy.load(<span class="st">'de_core_news_sm'</span>)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> [token.text.lower() <span class="cf">for</span> token <span class="kw">in</span> nlp(sentence)]</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> [token.lower() <span class="cf">for</span> token <span class="kw">in</span> sentence]</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> [src_field.init_token] <span class="op">+</span> tokens <span class="op">+</span> [src_field.eos_token]</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    src_indexes <span class="op">=</span> [src_field.vocab.stoi[token] <span class="cf">for</span> token <span class="kw">in</span> tokens]</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    src_tensor <span class="op">=</span> torch.LongTensor(src_indexes).unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>    src_mask <span class="op">=</span> model.make_src_mask(src_tensor)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>        enc_src <span class="op">=</span> model.encoder(src_tensor, src_mask)</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>    trg_indexes <span class="op">=</span> [trg_field.vocab.stoi[trg_field.init_token]]</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_len):</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>        trg_tensor <span class="op">=</span> torch.LongTensor(trg_indexes).unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>        trg_mask <span class="op">=</span> model.make_trg_mask(trg_tensor)</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>            output, attention <span class="op">=</span> model.decoder(trg_tensor, enc_src, trg_mask, src_mask)</span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>        pred_token <span class="op">=</span> output.argmax(<span class="dv">2</span>)[:,<span class="op">-</span><span class="dv">1</span>].item()</span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>        trg_indexes.append(pred_token)</span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pred_token <span class="op">==</span> trg_field.vocab.stoi[trg_field.eos_token]:</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>    trg_tokens <span class="op">=</span> [trg_field.vocab.itos[i] <span class="cf">for</span> i <span class="kw">in</span> trg_indexes]</span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trg_tokens[<span class="dv">1</span>:], attention</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ahora definiremos una función que muestra la atención sobre la oración fuente para cada paso de la decodificación. Como este modelo tiene 8 cabezas, nuestro modelo puede ver la atención de cada una de las cabezas.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:50.328867Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:50.301819Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;633fd0f931f355f498d4f2c68e359571&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-6b7fc6ca641ebfa8&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="38">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> display_attention(sentence, translation, attention, n_heads <span class="op">=</span> <span class="dv">8</span>, n_rows <span class="op">=</span> <span class="dv">4</span>, n_cols <span class="op">=</span> <span class="dv">2</span>):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> n_rows <span class="op">*</span> n_cols <span class="op">==</span> n_heads</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">25</span>))</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_heads):</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> fig.add_subplot(n_rows, n_cols, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>        _attention <span class="op">=</span> attention.squeeze(<span class="dv">0</span>)[i].cpu().detach().numpy()</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>        cax <span class="op">=</span> ax.matshow(_attention, cmap<span class="op">=</span><span class="st">'bone'</span>)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>        ax.tick_params(labelsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>        ax.set_xticklabels([<span class="st">''</span>]<span class="op">+</span>[<span class="st">'&lt;sos&gt;'</span>]<span class="op">+</span>[t.lower() <span class="cf">for</span> t <span class="kw">in</span> sentence]<span class="op">+</span>[<span class="st">'&lt;eos&gt;'</span>], </span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>                           rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>        ax.set_yticklabels([<span class="st">''</span>]<span class="op">+</span>translation)</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>        ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="dv">1</span>))</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>        ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="dv">1</span>))</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>    plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ahora es momento de probar nuestro modelo! 😁</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:50.344977Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:50.331873Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;2815332b5bfdf751d6f19a3e086d4fc7&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-463d9fe9ecb982ab&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="39">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>example_idx <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> <span class="bu">vars</span>(train_data.examples[example_idx])[<span class="st">'src'</span>]</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>trg <span class="op">=</span> <span class="bu">vars</span>(train_data.examples[example_idx])[<span class="st">'trg'</span>]</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'src = </span><span class="sc">{</span>src<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'trg = </span><span class="sc">{</span>trg<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>src = ['eine', 'frau', 'mit', 'einer', 'großen', 'geldbörse', 'geht', 'an', 'einem', 'tor', 'vorbei', '.']
trg = ['a', 'woman', 'with', 'a', 'large', 'purse', 'is', 'walking', 'by', 'a', 'gate', '.']</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:50.408869Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:50.348909Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;8cba2824cabf5beaa5ba9e15cf101945&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-1e0cd497db8e5f26&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="40">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>translation, attention <span class="op">=</span> translate_sentence(src, SRC, TRG, model, device)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'predicted trg = </span><span class="sc">{</span>translation<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>predicted trg = ['a', 'woman', 'with', 'a', 'large', 'purse', 'walks', 'past', 'a', 'gate', '.', '&lt;eos&gt;']</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:50.424053Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:50.409827Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;102bc43e309817fbce7df20cb31823ad&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-0096674ed0d450cb&quot;,&quot;locked&quot;:true,&quot;points&quot;:50,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="41">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">50</span>):        </span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_lists_by_percentage(trg, translation, <span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"50"}--> 
         ✓ [50 marks] 
         </h1> </div>
</div>
</div>
<p>Podemos ver la atención de cada cabeza a continuación. Cada uno es ciertamente diferente, pero es difícil (quizás imposible) razonar sobre a qué ha aprendido realmente la cabeza a prestar atención. Algunas cabezas prestan toda su atención a “eine” cuando traducen “a”, otras no lo hacen en absoluto y otras un poco. Todos parecen seguir el patrón similar de “escalera descendente” y la atención al emitir los dos últimos tokens se distribuye por igual entre los dos últimos tokens en la oración de entrada.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:52.343229Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:50.432036Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;cfebe931c932e3ccd6c13703688e49a5&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-7e9c2e8fe816190e&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="42">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>display_attention(src, translation, attention)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\aleja\AppData\Local\Temp\ipykernel_17780\3470126203.py:16: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_xticklabels(['']+['&lt;sos&gt;']+[t.lower() for t in sentence]+['&lt;eos&gt;'],
C:\Users\aleja\AppData\Local\Temp\ipykernel_17780\3470126203.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_yticklabels(['']+translation)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Lab5_files/figure-html/cell-42-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:52.358493Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:52.345290Z&quot;}" data-execution_count="43">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>example_idx <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> <span class="bu">vars</span>(valid_data.examples[example_idx])[<span class="st">'src'</span>]</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>trg <span class="op">=</span> <span class="bu">vars</span>(valid_data.examples[example_idx])[<span class="st">'trg'</span>]</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'src = </span><span class="sc">{</span>src<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'trg = </span><span class="sc">{</span>trg<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>src = ['ein', 'kleiner', 'junge', 'mit', 'einem', 'giants-trikot', 'schwingt', 'einen', 'baseballschläger', 'in', 'richtung', 'eines', 'ankommenden', 'balls', '.']
trg = ['a', 'young', 'boy', 'wearing', 'a', 'giants', 'jersey', 'swings', 'a', 'baseball', 'bat', 'at', 'an', 'incoming', 'pitch', '.']</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:52.466396Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:52.359502Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;98d428ddb423d9b272c053e423cdb138&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-166898cddb8d99b7&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="44">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>translation, attention <span class="op">=</span> translate_sentence(src, SRC, TRG, model, device)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'predicted trg = </span><span class="sc">{</span>translation<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>predicted trg = ['a', 'little', 'boy', 'wearing', 'a', 'baseball', 'cap', 'swings', 'a', 'ball', 'toward', 'a', 'ball', 'in', 'an', 'alley', '.', '&lt;eos&gt;']</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:52.482189Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:52.467354Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;4ea4979922f6b8effb38b064e6f083b3&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-286cfb2414e68353&quot;,&quot;locked&quot;:true,&quot;points&quot;:50,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="45">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">50</span>):        </span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_lists_by_percentage(trg, translation, <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\Users\aleja\Documents\GitHub\Lab5_DL\lab5\Lib\site-packages\lautils\gradeutils.py:38: UserWarning: List are of different size.
  warnings.warn("List are of different size.", UserWarning)</code></pre>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"50"}--> 
         ✓ [50 marks] 
         </h1> </div>
</div>
</div>
<p>Una vez más, algunas cabezas prestan toda su atención a “ein”, mientras que otras no le prestan atención. Una vez más, la mayoría de los heads parecen extender su atención sobre los tokens de punto y <eos> en la oración de origen cuando emiten el punto y la oración <eos> en la oración de destino predicha, aunque algunos parecen prestar atención a los tokens cerca del comienzo de la oración.</eos></eos></p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:54.624539Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:52.484329Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;80eb096b5eea5a0e65e4e856f8a05ab6&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-0fd7fff7238131b9&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="46">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>display_attention(src, translation, attention)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\aleja\AppData\Local\Temp\ipykernel_17780\3470126203.py:16: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_xticklabels(['']+['&lt;sos&gt;']+[t.lower() for t in sentence]+['&lt;eos&gt;'],
C:\Users\aleja\AppData\Local\Temp\ipykernel_17780\3470126203.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_yticklabels(['']+translation)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Lab5_files/figure-html/cell-46-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:54.639714Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:54.626442Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;bb7e710ca5e77afa27d1785edcfe394e&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-967df8f204a0fac5&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="47">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>example_idx <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> <span class="bu">vars</span>(test_data.examples[example_idx])[<span class="st">'src'</span>]</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>trg <span class="op">=</span> <span class="bu">vars</span>(test_data.examples[example_idx])[<span class="st">'trg'</span>]</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'src = </span><span class="sc">{</span>src<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'trg = </span><span class="sc">{</span>trg<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>src = ['eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.']
trg = ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:54.748290Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:54.640704Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;36992ec8a02f0339d4a31d4b491fe0ec&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-2a6447254fdf9660&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="48">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>translation, attention <span class="op">=</span> translate_sentence(src, SRC, TRG, model, device)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'predicted trg = </span><span class="sc">{</span>translation<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>predicted trg = ['a', 'mother', 'and', 'her', 'baby', 'enjoying', 'a', 'nice', 'day', 'outdoors', '.', '&lt;eos&gt;']</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:54.763438Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:54.750339Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;c7d70cc14a992f22862a6dbe4d8817e0&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-81a863f7f367d638&quot;,&quot;locked&quot;:true,&quot;points&quot;:50,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="49">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">50</span>):        </span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_lists_by_percentage(trg, translation, <span class="fl">33.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"50"}--> 
         ✓ [50 marks] 
         </h1> </div>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:56.816015Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:54.766636Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;ea3bef5af18ac40a93d9eb1615c29639&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-c07cb66d79e0d46d&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="50">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>display_attention(src, translation, attention)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\aleja\AppData\Local\Temp\ipykernel_17780\3470126203.py:16: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_xticklabels(['']+['&lt;sos&gt;']+[t.lower() for t in sentence]+['&lt;eos&gt;'],
C:\Users\aleja\AppData\Local\Temp\ipykernel_17780\3470126203.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax.set_yticklabels(['']+translation)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Lab5_files/figure-html/cell-50-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Calculamos el score BLEU</p>
<p><strong>NB:</strong> El score BLEU (Bilingual Evaluation Understudy) es una métrica para evaluar la calidad de las traducciones generadas por máquinas en comparación con referencias humanas. Mide la superposición de secuencias de n-gramas entre la traducción generada por la máquina y las traducciones de referencia. BLEU calcula la precisión contando los n-gramas coincidentes y también aplica una penalización por brevedad para fomentar traducciones más largas. Produce un puntaje entre 0 y 1, siendo puntajes más altos indicativos de una mejor calidad de traducción, aunque no captura todas las sutilezas de la calidad de la traducción.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:44:56.831840Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:56.817720Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;0046279276405f303b20cc2459a3c7c8&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-03a3ba3ea92b0b4b&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="51">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.data.metrics <span class="im">import</span> bleu_score</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_bleu(data, src_field, trg_field, model, device, max_len <span class="op">=</span> <span class="dv">50</span>):</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    trgs <span class="op">=</span> []</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>    pred_trgs <span class="op">=</span> []</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> datum <span class="kw">in</span> data:</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>        src <span class="op">=</span> <span class="bu">vars</span>(datum)[<span class="st">'src'</span>]</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>        trg <span class="op">=</span> <span class="bu">vars</span>(datum)[<span class="st">'trg'</span>]</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>        pred_trg, _ <span class="op">=</span> translate_sentence(src, src_field, trg_field, model, device, max_len)</span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">#cut off &lt;eos&gt; token</span></span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>        pred_trg <span class="op">=</span> pred_trg[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a>        pred_trgs.append(pred_trg)</span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a>        trgs.append([trg])</span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> bleu_score(pred_trgs, trgs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:46:04.020343Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:44:56.833852Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;edaa71e73d67db478b2c90aa66babde3&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-015f787de4c116b3&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="52">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>bleu_score_ <span class="op">=</span> calculate_bleu(test_data, SRC, TRG, model, device)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'BLEU score = </span><span class="sc">{</span>bleu_score_<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>BLEU score = 35.66</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:49:58.969506Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:49:58.950502Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;4488f6d31ff48161e210350022faa25c&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-63d81743200ae3e4&quot;,&quot;locked&quot;:true,&quot;points&quot;:50,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="53">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">50</span>):        </span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_numbers(new_representation(bleu_score_), <span class="st">"3e3d"</span>, <span class="st">'0x1.5c28f5c28f5c3p-2'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"50"}--> 
         ✓ [50 marks] 
         </h1> </div>
</div>
</div>
<p><strong>PREGUNTAS:</strong> Responda las siguintes preguntas en este espacio (10% de la nota) * ¿Cómo afecta la cantidad de parámetros del modelo? ¿Qué nos dicen eso 9M de parametros del modelo que hemos creado? - La cantidad de parametros nos indica la capacidad que tiene el modelo para aprender de los datos. Un modelo con una gran cantidad de parametros significa que puede aprender más relaciones complejas entre los datos. Al tener 9M de parametros el modelo puede indetificar conexiones entre las palabras de las oraciones y así poder traducir de mejor manera. * ¿Qué hace el algoritmo de inicialización de Xavier Uniform? - La inicialización de Xavier es utilizada para inicializar los pesos de las redes neuronales. Esta inicialización se basa en la distribución de los pesos de la red, para que estos no sean ni muy grandes ni muy pequeños. Esto permite que la red pueda aprender de mejor manera. Al ser Xavier Uniform, los pesos son inicializados de manera aleatoria con una distribución uniforme. * ¿Qué hace el comando torch.no_grad()? - Este comando se utiliza para deshabilitar el cálculo de gradientes. Esto es útil cuando se quiere evaluar el modelo, ya que no se necesita calcular los gradientes. * Interprete el valor obtenido para el BLEU score ¿es nuestro modelo un buen modelo? - El valor obtenido para el BLEU score es de 35.66 lo que muestra que el modelo es aceptable. Sin embargo, el modelo puede que cometa errores o no traduzca de manera correcta algunas oraciones. * ¿Qué puede observar de las palabras donde el modelo se ha confundido? - En las tres instacias de prueba el modelo comete errores en palabras que comparten el mismo significado. Esto no indica que sean palabras incorrectas solo palabras que se podrían utilizar en el contexto de la oración. * Observe el comportamiento de la pérdida y PPL en training y validation mientras se entrega el modelo, ¿qué puede decir de estos valores? - La pérdida y PPL en training y validation disminuyen a medida que se entrena el modelo. Esto indica que el modelo está aprendiendo de mejor manera a traducir las oraciones. * Si bien no es una tarea intuitiva o sencilla la interpretación de las gráficas de attention que hemos realizado, intente darle una interpretación a la última de estas gráficas mostrada. ¿Qué tipo de insights podría sacar de esta gráfica? - En la gráfica se puede observar que el modelo se enfoca en las palabras que tienen un mayor peso en la oración. Esto permite que el modelo pueda traducir de mejor manera las oraciones. - En la grafica tambien se puede observar si la detección de patrones es consistente en diversas partes de la entrada. - En la grafica se puede ver si el modelo presta más atención a las palabras clave de la oración.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-15T00:58:18.044718Z&quot;,&quot;start_time&quot;:&quot;2023-08-15T00:58:18.032219Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;53780b870ba4e5232e278b64b641f336&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-c5784ed59a9b5cb2&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="54">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio"</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>tick.summarise_marks() <span class="co"># </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio</code></pre>
</div>
<div class="cell-output cell-output-display">
<!--{id:"TOTALMARK",marks:"250", available:"250"}  -->
        
        <h1> 250 / 250 marks (100.0%) </h1>
        
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>